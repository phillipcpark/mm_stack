import numpy as np
import sys
import tensorflow as tf
import copy
import scipy.stats as scistats

#
# normal model implementation
#
def asmb_lstm(hp, dropout=True):
    seq_len     = hp["seq_len"]
    target_len  = hp["target_len"]
    feat_dim    = hp["feat_dim"]
    label_dim   = hp["label_dim"]
    hidden_dims = hp["hidden_dims"]
    clip_thresh = hp["clip_thresh"]
  
    #data feeds (allows for variable batch sz)
    feat_bat_ph  = tf.placeholder(tf.float32, [None, seq_len, feat_dim])  
    label_bat_ph = tf.placeholder(tf.float32, [None, target_len, label_dim])  
   
    #create LSTM cells/layers
    cells           = [tf.nn.rnn_cell.LSTMCell(hidden_dim, state_is_tuple=True) for hidden_dim in hidden_dims]  
    multi_lstm_cell = tf.nn.rnn_cell.MultiRNNCell(cells)

    #loss is computed from last output generated by sequence
    output, last_state = tf.nn.dynamic_rnn(multi_lstm_cell, feat_bat_ph, dtype=tf.float32)
    loss               = tf.reduce_mean(tf.squared_difference(output[:,-target_len:], label_bat_ph)) 

    #compute gradient, clip, and run through optimizer
    _optimizer         = tf.train.AdamOptimizer()
    grad, wrt_params   = zip(*_optimizer.compute_gradients(loss))
    clipped, glob_norm = tf.clip_by_global_norm(grad, clip_thresh) 
    optimizer          = _optimizer.apply_gradients(zip(clipped, wrt_params))
    
    graph_refs = {"feat_bat_ph":feat_bat_ph, "label_bat_ph":label_bat_ph, "multi_lstm_cell":multi_lstm_cell, "output":output, \
                  "loss":loss, "optimizer":optimizer}
    return graph_refs
  
#
# train model and return error [batch error, averaged epoch error, validation error, average validation error]
#
def train_for_err(hp, sess_specs, lstm_graph, tr_data):
    epochs      = hp["epochs"]
    batch_count = hp["bat_count"]

    tr_feat_bats     = tr_data["tr_set"]["feat_bats"]
    tr_label_bats    = tr_data["tr_set"]["label_bats"]
    valid_feat_bats  = tr_data["tst_set"]["feat_bats"]   
    valid_label_bats = tr_data["tst_set"]["label_bats"]

    #NOTE
    #os.environ['CUDA_VISIBLE_DEVICES'] = ''
 
    init_op = tf.global_variables_initializer()
    #sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
    sess    = tf.Session()
    sess.run(init_op)
   
    bat_err = []
    avg_ep_err  = [] #averaged, repeated batch_count # of times 
    valid_err = []
    avg_valid_err = []

    #NOTE
    #_saver = tf.train.Saver()  
    #_saver.save(sess, '../params_cp/subseq_262144_bat_128/initial', global_step=0)

    for ep_idx in range(epochs):
        ep_err = []

        for bat_idx in range(batch_count): 
            feat_bat  = tr_feat_bats[bat_idx]
            label_bat = tr_label_bats[bat_idx]
           
            ep_err.append(sess.run(lstm_graph["loss"], {lstm_graph["feat_bat_ph"]:feat_bat, lstm_graph["label_bat_ph"]:label_bat})) 
            sess.run(lstm_graph["optimizer"], {lstm_graph["feat_bat_ph"]:feat_bat, lstm_graph["label_bat_ph"]:label_bat})       
   
            if (ep_idx % 5 == 0):
                bat_loss = sess.run(lstm_graph["loss"], {lstm_graph["feat_bat_ph"]:feat_bat, lstm_graph["label_bat_ph"]:label_bat})
                print("ep_" + str(ep_idx) + " " + str(bat_loss))   

        #if (ep_idx % 100 == 0 and ep_idx != 0):
        #    _saver.save(sess, '../params_cp/subseq_262144_bat_128/CP_', global_step=ep_idx)

 
        '''
        #pad average epoch error to align with per-example error
        ep_mean = np.mean(ep_err)  
        for bat_pos in range(hp["bat_count"]):
            avg_ep_err.append(ep_mean)
        bat_err += ep_err

        #validation error for epoch
        ep_valid_err = []
        for valid_idx in range(len(valid_feat_bats)):
            feat_bat = valid_feat_bats[valid_idx]
            label_bat = valid_label_bats[valid_idx]      

            ep_valid_err.append(sess.run(lstm_graph["loss"], {lstm_graph["feat_bat_ph"]:feat_bat, lstm_graph["label_bat_ph"]:label_bat})) 
        valid_err += ep_valid_err                   

        #fit average per-epoch validation error to same time scale as per-example validation error
        ep_avg_valid_err = np.mean(ep_valid_err)
        ep_avg_valid_err = [ep_avg_valid_err for bat_idx in range(len(valid_feat_bats))] 
        avg_valid_err   += ep_avg_valid_err
        '''   
    #all_err = [bat_err, avg_ep_err, valid_err, avg_valid_err]

    #put squared error into range
    #for err in all_err:
    #    np.sqrt(err)
 
    #return sess, [bat_err, avg_ep_err, valid_err, avg_valid_err] 
    return sess
 
#
# k-fold validation 
#
def kf_valid(hp, lstm_graph, sess_dat):
    epochs             = hp["epochs"]
    total_tr_bat_count = hp["bat_count"]
    kfold_sets         = sess_dat["tr_set"]
  
    all_fold_errs = []

    for kf_set_idx in range(len(kfold_sets)):
        print("fold " + str(kf_set_idx))

        feat_bats  = kfold_sets[kf_set_idx]["tr_set"]["feat_bats"]
        label_bats = kfold_sets[kf_set_idx]["tr_set"]["label_bats"]

        valid_feat_bats  = kfold_sets[kf_set_idx]["valid_set"]["feat_bats"]
        valid_label_bats = kfold_sets[kf_set_idx]["valid_set"]["label_bats"]
        valid_bat_count  = len(valid_feat_bats)
 
        init_op = tf.global_variables_initializer()
        sess    = tf.Session()
        sess.run(init_op)
      
        for ep_idx in range(epochs):   
            for bat_idx in range(total_tr_bat_count - valid_bat_count): 
                feat_bat  = feat_bats[bat_idx]
                label_bat = label_bats[bat_idx]
                  
                sess.run(lstm_graph["optimizer"], {lstm_graph["feat_bat_ph"]:feat_bat, lstm_graph["label_bat_ph"]:label_bat})       
       
                if (ep_idx % 100 == 0):
                    bat_loss = sess.run(lstm_graph["loss"], {lstm_graph["feat_bat_ph"]:feat_bat, lstm_graph["label_bat_ph"]:label_bat})
                    print("ep_" + str(ep_idx) + " " + str(bat_loss))   

        #run validation sets
        fold_err = []
        for bat_idx in range(len(valid_feat_bats)):
            feat_bat  = valid_feat_bats[bat_idx]
            label_bat = valid_label_bats[bat_idx]    
 
            valid_bat_err = sess.run(lstm_graph["loss"], {lstm_graph["feat_bat_ph"]:feat_bat, lstm_graph["label_bat_ph"]:label_bat})
            fold_err.append(valid_bat_err)

        avg_fold_err = np.mean(fold_err)         
        all_fold_errs.append(avg_fold_err)   

    model_valid_err = np.mean(all_fold_errs) 
    return model_valid_err 

        
#
# get predictions from trained model using both training and test sets
#
def test_predicts(hp, tf_sess, lstm_graph, sess_dat, restore_path=None):
    ts_samps = sess_dat["samples"]["ts"]
    mp_samps = sess_dat["samples"]["mp"]
   
    target_len = hp["target_len"]
    predicts   = []
   
    if (restore_path != None):
        print("\n**restoring checkpoint weights")
        _saver = tf.train.Saver()
        _saver.restore(tf_sess, restore_path)
   
    #sess_offset = 12958720
    bat_sz    = hp["bat_sz"]
    bat_count = hp["bat_count"] 
    seq_len   = hp["seq_len"]
 
    for bat_idx in range(bat_count):
        if (bat_idx % 100 == 0):
            print("**feeding batch " + str(bat_idx) + " of " + str(bat_count))

        feat_bat  = [[[samp] for samp in ts_samps[(bat_idx*bat_sz)+seq_idx:(bat_idx*bat_sz)+seq_idx+seq_len]] for seq_idx in range(bat_sz)]                                   
        label_bat = [[[mp_samps[bat_idx*bat_sz+seq_idx]]] for seq_idx in range(bat_sz)] 

        bat_predicts = tf_sess.run(lstm_graph["output"], \
                                         {lstm_graph["feat_bat_ph"]:feat_bat, lstm_graph["label_bat_ph"]:label_bat})

        predicts += [seq_predict[0][0] for seq_predict in bat_predicts[:, -target_len:]]
    return predicts






