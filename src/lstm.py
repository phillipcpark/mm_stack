import numpy as np
import sys
import tensorflow as tf
import copy
import scipy.stats as scistats

#
# returns lstm
#
def asmb_lstm(hp):
    gd_clip_thresh = 0.5
  
    feat_bat  = tf.placeholder(tf.float32, [None, hp["subseq_len"], hp["feat_dim"]])  
    label_bat = tf.placeholder(tf.float32, [None, 1, hp["label_dim"]])  
   
    #create LSTM cells/layers
    cells      = [tf.nn.rnn_cell.LSTMCell(hidden_dim, state_is_tuple=True) for hidden_dim in hp["layer_dims"]]  
    lstm_cells = tf.nn.rnn_cell.MultiRNNCell(cells)

    #loss is computed from last output generated by sequence
    output, last_state = tf.nn.dynamic_rnn(lstm_cells, feat_bat, dtype=tf.float32)
    loss               = tf.reduce_mean(tf.squared_difference(output[:,-1:], label_bat)) 

    #compute gradient, clip, and run through optimizer
    _optimizer         = tf.train.AdamOptimizer()
    grad, wrt_params   = zip(*_optimizer.compute_gradients(loss))
    clipped, glob_norm = tf.clip_by_global_norm(grad, gd_clip_thresh) 
    optimizer          = _optimizer.apply_gradients(zip(clipped, wrt_params))
    
    graph_refs = {"feat_bat":feat_bat, "label_bat":label_bat, "lstm_cells":lstm_cells, "output":output, \
                  "loss":loss, "optimizer":optimizer}
    return graph_refs

#
#
#
def ensemb_from_src(lstm_graph, ensemb_count):

    src_params    = lstm_graph["lstm_cells"].get_weights()
    ensemb_params = [[np.array(np.copy(params)) for params in src_params] for ensemb in range(ensemb_count)]
       
    return ensemb_params

  
#
# train model and return error [batch error, averaged epoch error, validation error, average validation error]
#
def train_ensemble(hp, sess_data, _sess, lstm_graph, ensemb_params):
    feat_subseqs = sess_data["feat_subseqs"]
    labels       = sess_data["labels"] 

    with _sess.as_default() as sess:

        for comp_idx in range(hp["ensemb_sz"]):
            print("\n**training component model " + str(comp_idx))
    
            lstm_graph["lstm_cells"].set_weights(ensemb_params[comp_idx])
        
            bat_idxs = sess_data["bat_idxs"][comp_idx]
    
            print("**bat count: " + str(len(bat_idxs)))
            for ep in range(hp["epochs"]): 
                for bat_idx in range(len(bat_idxs)):
                    bat_feats  = [[[val] for val in feat_subseqs[_idx]] for _idx in bat_idxs[bat_idx]]  
                    bat_labels = [[[labels[_idx]]] for _idx in bat_idxs[bat_idx]]
    
                    if (ep % 5 == 0):
                        print("**ep: " + str(ep) + " " + str(np.sqrt(sess.run(lstm_graph["loss"], {lstm_graph["feat_bat"]:bat_feats, lstm_graph["label_bat"]:bat_labels}))))
                   
                    sess.run(lstm_graph["optimizer"], {lstm_graph["feat_bat"]:bat_feats, lstm_graph["label_bat"]:bat_labels})
    
            ensemb_params[comp_idx] = lstm_graph["lstm_cells"].get_weights()
    
   
    
    



