import numpy as np
import sys
import tensorflow as tf
import copy
import scipy.stats as scistats

#
# returns lstm
#
def asmb_lstm(hp):
    gd_clip_thresh = 0.5
  
    feat_bat  = tf.placeholder(tf.float32, [None, hp["subseq_len"], hp["feat_dim"]])  
    label_bat = tf.placeholder(tf.float32, [None, 1, hp["label_dim"]])  
   
    #create LSTM cells/layers
    cells      = [tf.nn.rnn_cell.LSTMCell(hidden_dim, state_is_tuple=True) for hidden_dim in hp["layer_dims"]]  
    lstm_cells = tf.nn.rnn_cell.MultiRNNCell(cells)

    #loss is computed from last output generated by sequence
    output, last_state = tf.nn.dynamic_rnn(lstm_cells, feat_bat, dtype=tf.float32)
    loss               = tf.reduce_mean(tf.squared_difference(output[:,-1:], label_bat)) 

    #compute gradient, clip, and run through optimizer
    _optimizer         = tf.train.AdamOptimizer()
    grad, wrt_params   = zip(*_optimizer.compute_gradients(loss))
    clipped, glob_norm = tf.clip_by_global_norm(grad, gd_clip_thresh) 
    optimizer          = _optimizer.apply_gradients(zip(clipped, wrt_params))
    
    graph_refs = {"feat_bat":feat_bat, "label_bat":label_bat, "lstm_cells":lstm_cells, "output":output, \
                  "loss":loss, "optimizer":optimizer}
    return graph_refs

#
#
#
def ensemb_from_src(lstm_graph, ensemb_count):

    src_params    = lstm_graph["lstm_cells"].get_weights()
    ensemb_params = [[np.array(np.copy(params)) for params in src_params] for ensemb in range(ensemb_count + 1)]
      
    #ref params, ensemb 
    return [ensemb_params[0]], ensemb_params[1:]

#
#
#
def train(hp, sess_data, _sess, lstm_graph, params):
    tr_subseqs = sess_data["tr_set"]["feat_subseqs"]
    tr_labels  = sess_data["tr_set"]["labels"] 

    print("\n**training single LSTM")

    with _sess.as_default() as sess:   
        lstm_graph["lstm_cells"].set_weights(params[0]) #params are always wrapped so that argument is pointer 
        bat_idxs = [bat for comp_bats in sess_data["bat_idxs"] for bat in comp_bats]
 
        print("**bat count: " + str(len(bat_idxs)))
        for ep in range(hp["epochs"]): 
            for bat_idx in range(len(bat_idxs)):
                bat_feats  = [[[val] for val in tr_subseqs[_idx]] for _idx in bat_idxs[bat_idx]]  
                bat_labels = [[[tr_labels[_idx]]] for _idx in bat_idxs[bat_idx]]

                #output RMSE every 5 epochs    
                if (ep % 5 == 0):
                    print("**ep: " + str(ep) + " RMSE: " + str(np.sqrt(sess.run(lstm_graph["loss"], {lstm_graph["feat_bat"]:bat_feats, lstm_graph["label_bat"]:bat_labels}))))
              
                sess.run(lstm_graph["optimizer"], {lstm_graph["feat_bat"]:bat_feats, lstm_graph["label_bat"]:bat_labels})
    
        params[0] = lstm_graph["lstm_cells"].get_weights()
 


  
#
# train model and return error [batch error, averaged epoch error, validation error, average validation error]
#
def train_ensemble(hp, sess_data, _sess, lstm_graph, ensemb_params):
    tr_subseqs = sess_data["tr_set"]["feat_subseqs"]
    tr_labels  = sess_data["tr_set"]["labels"] 

    with _sess.as_default() as sess:

        for comp_idx in range(hp["ensemb_sz"]):
            print("\n**training component model " + str(comp_idx))
    
            lstm_graph["lstm_cells"].set_weights(ensemb_params[comp_idx]) 
            bat_idxs = sess_data["bat_idxs"][comp_idx]
    
            print("**bat count: " + str(len(bat_idxs)))
            for ep in range(hp["epochs"]): 
                for bat_idx in range(len(bat_idxs)):
                    bat_feats  = [[[val] for val in tr_subseqs[_idx]] for _idx in bat_idxs[bat_idx]]  
                    bat_labels = [[[tr_labels[_idx]]] for _idx in bat_idxs[bat_idx]]

                    #output RMSE every 5 epochs    
                    if (ep % 5 == 0):
                        print("**ep: " + str(ep) + " RMSE: " + str(np.sqrt(sess.run(lstm_graph["loss"], {lstm_graph["feat_bat"]:bat_feats, lstm_graph["label_bat"]:bat_labels}))))
                  
                    sess.run(lstm_graph["optimizer"], {lstm_graph["feat_bat"]:bat_feats, lstm_graph["label_bat"]:bat_labels})
    
            ensemb_params[comp_idx] = lstm_graph["lstm_cells"].get_weights()


#
#
#
def test_lstm(hp, sess_data, _sess, lstm_graph, params):
    tst_subseqs = sess_data["tst_set"]["feat_subseqs"]
    tst_labels  = sess_data["tst_set"]["labels"] 

    print("\n**testing single LSTM")

    with _sess.as_default() as sess: 
        _tst_subseqs = [ [[[val] for val in subseq]] for subseq in tst_subseqs]  
        _tst_labels  = [ [[[val]]] for val in tst_labels]

        tst_err = []
        lstm_graph["lstm_cells"].set_weights(params[0])   

        for tst_idx in range(len(_tst_subseqs[:500])):
            print("\n**test_idx: " + str(tst_idx) + " of " + str(len(_tst_subseqs)))
                
            out = sess.run(lstm_graph["output"][0][-1], {lstm_graph["feat_bat"]: _tst_subseqs[tst_idx], lstm_graph["label_bat"]: _tst_labels[tst_idx]}) 
            loss = np.abs(out - tst_labels[tst_idx])

            tst_err.append(loss)
        return tst_err

         
#
# train model and return error [batch error, averaged epoch error, validation error, average validation error]
#
def test_ensemble(hp, sess_data, _sess, lstm_graph, ensemb_params, gmm):
    tst_subseqs = sess_data["tst_set"]["feat_subseqs"]
    tst_labels  = sess_data["tst_set"]["labels"] 

    print("\n**testing ensemble")

    with _sess.as_default() as sess: 
        _tst_subseqs = [ [[[val] for val in subseq]] for subseq in tst_subseqs]  
        _tst_labels  = [ [[[val]]] for val in tst_labels]

        tst_err = []

        #FIXME only partial test
        for tst_idx in range(len(_tst_subseqs[:500])):
            print("\n**test_idx: " + str(tst_idx) + " of " + str(len(_tst_subseqs)))
            comp_outs = []              

            for comp_idx in range(hp["ensemb_sz"]):
                print("comp_idx: " + str(comp_idx))    

                lstm_graph["lstm_cells"].set_weights(ensemb_params[comp_idx])        
                comp_out = sess.run(lstm_graph["output"][0][-1], {lstm_graph["feat_bat"]: _tst_subseqs[tst_idx], lstm_graph["label_bat"]: _tst_labels[tst_idx]})
     
                comp_outs.append(comp_out[0])
           
            comp_densities = [val for val in gmm.predict_proba([tst_subseqs[tst_idx]])[0]]

            _h = np.dot(comp_outs, comp_densities)
            loss = np.abs(_h - tst_labels[tst_idx])

            tst_err.append(loss)
        return tst_err

